{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Spam Detection\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: 10\n",
    "\n",
    "Complete this notebook and submit it. The notebook needs to be a complete project report with \n",
    "\n",
    "* your implementation,\n",
    "* documentation including a short discussion of how your implementation works and your design choices, and\n",
    "* experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. \n",
    "\n",
    "Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A spam detection agent gets as its percepts text messages and needs to decide if they are spam or not.\n",
    "Create a [naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) for the \n",
    "[UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) to perform this task.\n",
    "\n",
    "__About the use of libraries:__ The point of this exercise is to learn how a Bayes classifier is built. You may use libraries for tokenizing, stop words and to create a document-term matrix, but you need to implement parameter estimation and prediction yourself.\n",
    "\n",
    "## Create a bag-of-words representation of the text messages [3 Points]\n",
    "\n",
    "The first step is to tokenize the text. Here is an example of how to use the [natural language tool kit (nltk)](https://www.nltk.org/) to create tokens (separate words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text message: \"ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\"\n",
      "tokens: ['ham', 'Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# You need to install nltk and then download the tokenizer once.\n",
    "# nltk.download('punkt')\n",
    "\n",
    "file = open(\"smsspamcollection/SMSSpamCollection\", \"r\")\n",
    "\n",
    "sentence = file.readline()\n",
    "print(f\"text message: \\\"{sentence}\\\"\")\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(f\"tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with removing frequent words (called [stopwords](https://en.wikipedia.org/wiki/Stop_word)) and very infrequent words so you end up with a reasonable number of words used in the classifier. Maybe you need to remove digits or all non-letter characters. You may also use a stemming algorithm. \n",
    "\n",
    "Convert the tokenized data into a data structure that indicates for each for document what words it contains. The data structure can be a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) with 0s and 1s, a pandas dataframe or some sparse matrix structure. Note: words, tokens and terms are often used interchangably. Make sure the data structure can be used to split the data into training and test documents (see below).\n",
    "\n",
    "Report the 20 most frequent and the 20 least frequent words in your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description and code goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn parameters [3 Points]\n",
    "\n",
    "Use 80% of the data (called training set; randomly chosen) to learn the parameters of the naive Bayes classifier (prior probabilities and likelihoods). \n",
    "Remember, the naive Bayes classifier calculates:\n",
    "\n",
    "$$P(spam|message) \\propto score_{spam}(message) = P(spam) \\prod_{i=1}^n P(w_i | spam)$$\n",
    "$$P(ham|message) \\propto score_{ham}(message) = P(ham) \\prod_{i=1}^n P(w_i | ham)$$\n",
    "\n",
    "and classifies a message as spam if \n",
    "$$score_{spam}(message) > score_{ham}(message).$$ \n",
    "\n",
    "You therefore need to\n",
    "estimate: \n",
    "\n",
    "* the priors $P(spam)$ and $P(ham)$, and \n",
    "* the likelihoods $P(w_i | spam)$ and $P(w_i | ham)$ for all words\n",
    "\n",
    "from counts obtained from the training data. Use [Laplacian smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to estimate the\n",
    "likelihoods. This deals with words that have very low count in the ham or spam messages and avoids\n",
    "likelihoods of zero.\n",
    "\n",
    "Report the top 20 words (highest conditional probability) for ham and for spam. These words represent the biggest clues that a message is ham or spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description and code goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the classification performance [4 Points] \n",
    "\n",
    "Classify the remaining 20% of the data (test set) and calculate classification accuracy. Accuracy is defined as the proportion of correctly classified test documents.\n",
    "\n",
    "1. How good is your classifier's accuracy compared to a baseline classifier.\n",
    "\n",
    "2. Inspect a few misclassified text messages and discuss why the classification failed.\n",
    "\n",
    "3. Discuss how you deal with words in the test data that you have not seen in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description, code and discussion goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task [+1 Point]\n",
    "\n",
    "Describe how you could improve the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
